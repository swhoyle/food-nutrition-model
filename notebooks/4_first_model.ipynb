{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "712723db",
      "metadata": {},
      "source": [
        "# Step 4: First Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "759a6f18",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Median imputation\n",
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "02b6849c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (384395, 9)\n",
            "Test shape: (51911, 9)\n"
          ]
        }
      ],
      "source": [
        "train_df = pd.read_csv(\"../data/train_processed.csv\")\n",
        "test_df = pd.read_csv(\"../data/test_processed.csv\")\n",
        "\n",
        "\n",
        "# First Model Training with KNN\n",
        "predictor = \"nutriscore_grade_encoded\"\n",
        "\n",
        "base_num = [\n",
        "    \"energy\",\"sugars\",\"carbohydrates\",\"salt\",\n",
        "    \"fat\",\"trans_fat\",\"proteins\",\"additives_n\",\"ingredients_n\"\n",
        "]\n",
        "num_cols = [c for c in base_num if c in train_df.columns]\n",
        "\n",
        "X_train = train_df[base_num].to_numpy()\n",
        "X_test = test_df[base_num].to_numpy()\n",
        "\n",
        "y_train = train_df[predictor].astype(int)\n",
        "y_test = train_df[predictor].astype(int)\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "Jn8pJmOFf0MZ",
      "metadata": {
        "id": "Jn8pJmOFf0MZ"
      },
      "outputs": [],
      "source": [
        "# KNN CLASSIFIER\n",
        "\n",
        "class KNNClassifier:\n",
        "    def __init__(self, k=7):\n",
        "        self.k = k\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def _distance(self, x1, x2):\n",
        "        # Euclidean distance\n",
        "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
        "\n",
        "    def predict_one(self, x):\n",
        "        # compute distances from x to ALL training samples\n",
        "        dists = np.linalg.norm(self.X_train - x, axis=1)\n",
        "\n",
        "        # find k nearest neighbors\n",
        "        k_idxs = np.argsort(dists)[:self.k]\n",
        "        k_labels = self.y_train[k_idxs]\n",
        "\n",
        "        # majority vote\n",
        "        values, counts = np.unique(k_labels, return_counts=True)\n",
        "        return values[np.argmax(counts)]\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self.predict_one(x) for x in X])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G6xDNgWzf0_o",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "G6xDNgWzf0_o",
        "outputId": "db63d886-eb6b-44b7-8ca3-0eaf4c7ff346"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "knn = KNNClassifier(k=7)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = knn.predict(X_train)\n",
        "y_test_pred = knn.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "train_acc = np.mean(y_train_pred == y_train)\n",
        "test_acc = np.mean(y_test_pred == y_test)\n",
        "\n",
        "print(f\"Train Accuracy: {train_acc:.3f}\")\n",
        "print(f\"Test Accuracy:  {test_acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QjU6sT7Yl5hE",
      "metadata": {
        "id": "QjU6sT7Yl5hE"
      },
      "source": [
        "Because the predict() implementation iterates across all 340,471 training samples for each of the 85,118 test samples, the computational cost becomes prohibitive and the end-to-end inference can require several hours.\n",
        "\n",
        "To identify potential optimizations, we applied GENAI-assisted research. The resulting improvement replaces nested Python loops with fully vectorized distance computations and batch-based prediction. Consequently, the K-Nearest Neighbors classifier achieves approximately significant reduction in runtime, as distance calculation is performed on entire matrices rather than iterating over each training instance for every test instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "B4Wbl17diLJ3",
      "metadata": {
        "id": "B4Wbl17diLJ3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class KNNClassifierScratch:\n",
        "\n",
        "    # NumPy-only KNN with batched L2 distances.\n",
        "    def __init__(self, k=7):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = np.asarray(X, dtype=np.float64)\n",
        "        self.y_train = np.asarray(y)\n",
        "        # Precompute squared norms for fast L2 distances\n",
        "        self.train_norm2 = np.einsum('ij,ij->i', self.X_train, self.X_train)\n",
        "        self.max_label = int(self.y_train.max())\n",
        "\n",
        "    def _predict_batch(self, Xb):\n",
        "        Xb = np.asarray(Xb, dtype=np.float64)\n",
        "        xb_norm2 = np.einsum('ij,ij->i', Xb, Xb)            # (B,)\n",
        "        cross = Xb @ self.X_train.T                         # (B, N)\n",
        "        d2 = xb_norm2[:, None] + self.train_norm2[None, :] - 2.0 * cross\n",
        "        np.maximum(d2, 0.0, out=d2)                         # safety\n",
        "\n",
        "        # top-k indices per row (no full sort)\n",
        "        idx_part = np.argpartition(d2, self.k-1, axis=1)[:, :self.k]    # (B, k)\n",
        "        row_idx = np.arange(Xb.shape[0])[:, None]\n",
        "        nearest_k = idx_part[row_idx, np.argsort(d2[row_idx, idx_part], axis=1)]\n",
        "\n",
        "        # majority vote via bincount\n",
        "        preds = np.empty(Xb.shape[0], dtype=self.y_train.dtype)\n",
        "        for i in range(Xb.shape[0]):\n",
        "            labs = self.y_train[nearest_k[i]]\n",
        "            bc = np.bincount(labs, minlength=self.max_label+1)\n",
        "            preds[i] = bc.argmax()\n",
        "        return preds\n",
        "\n",
        "    def predict(self, X, batch_size=128):\n",
        "        X = np.asarray(X, dtype=np.float64)\n",
        "        out = np.empty(X.shape[0], dtype=self.y_train.dtype)\n",
        "        for s in range(0, X.shape[0], batch_size):\n",
        "            e = min(s + batch_size, X.shape[0])\n",
        "            out[s:e] = self._predict_batch(X[s:e])\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "r7pRs4tuicvR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7pRs4tuicvR",
        "outputId": "8a7ed406-b888-4dc9-abdc-29fba61f7e6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[KNN k=7] Test Accuracy: 0.5009\n"
          ]
        }
      ],
      "source": [
        "knn = KNNClassifierScratch(k=7)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_test_pred = knn.predict(X_test, batch_size=128)\n",
        "\n",
        "test_acc = (y_test_pred == y_test).mean()\n",
        "print(f\"[KNN k=7] Test Accuracy: {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "CW0m3bUQoRlK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW0m3bUQoRlK",
        "outputId": "cfa83ec2-46e4-49e1-ef04-c7a2db5173d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[KNN k=7] Train Accuracy (20k subsample): 0.6130\n",
            "[KNN k=7] Test Accuracy:               0.5009\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Train Accuracy (subsample of train set)\n",
        "rng = np.random.default_rng(42)\n",
        "sub_n = min(20000, X_train.shape[0])\n",
        "idx_sub = rng.choice(X_train.shape[0], size=sub_n, replace=False)\n",
        "\n",
        "X_train_sub = X_train[idx_sub]\n",
        "y_train_sub = y_train[idx_sub]\n",
        "\n",
        "# Predict on this subsample\n",
        "y_train_sub_pred = knn.predict(X_train_sub, batch_size=128)\n",
        "train_acc = accuracy_score(y_train_sub, y_train_sub_pred)\n",
        "\n",
        "print(f\"[KNN k=7] Train Accuracy (20k subsample): {train_acc:.4f}\")\n",
        "print(f\"[KNN k=7] Test Accuracy:               {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ub9uTlHWpbNp",
      "metadata": {
        "id": "Ub9uTlHWpbNp"
      },
      "source": [
        "Metric\tResult:\n",
        "\n",
        "Train Accuracy (20k subsample)\t0.6130 (~61.3%)\n",
        "\n",
        "Test Accuracy\t0.5009 (~50.1%)\n",
        "\n",
        "Model\tKNN (k=7), implemented\n",
        "\n",
        "The model performs significantly better on training than on testing.\n",
        "\n",
        "The ~11% drop (61.3% → 50.1%) means the model learns patterns in training data that do not generalize perfectly.\n",
        "\n",
        "This indicates mild overfitting, but not extreme.\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "We implemented a K-Nearest Neighbors classifier using only NumPy, without using any scikit-learn classifiers. The model used scaled nutrient features and was evaluated using a train/test split.\n",
        "\n",
        "The higher training accuracy indicates the model fits the data reasonably well. However, the drop in test accuracy shows the model does not generalize perfectly to unseen samples, which suggests some overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "eAoUgFFSryUC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAoUgFFSryUC",
        "outputId": "7a2f8776-98d0-46ae-a607-c88d9f8e2d0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Evaluating KNN (k=1) ---\n",
            "Train Accuracy (20k subsample): 0.9279\n",
            "Test  Accuracy:                 0.5063\n",
            "\n",
            "--- Evaluating KNN (k=7) ---\n",
            "Train Accuracy (20k subsample): 0.6130\n",
            "Test  Accuracy:                 0.5009\n",
            "\n",
            "--- Evaluating KNN (k=15) ---\n",
            "Train Accuracy (20k subsample): 0.5624\n",
            "Test  Accuracy:                 0.4967\n",
            "\n",
            "Summary of K values:\n",
            "    k  train_accuracy  test_accuracy\n",
            "0   1          0.9279       0.506344\n",
            "1   7          0.6130       0.500893\n",
            "2  15          0.5624       0.496710\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "results = []  # to store (k, train_acc, test_acc)\n",
        "\n",
        "for k in [1, 7, 15]:\n",
        "\n",
        "    print(f\"\\n--- Evaluating KNN (k={k}) ---\")\n",
        "\n",
        "    knn = KNNClassifierScratch(k=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    # Test predictions (fast batched)\n",
        "    y_test_pred = knn.predict(X_test, batch_size=128)\n",
        "    test_acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    # Train accuracy on a subsample to avoid O(N^2)\n",
        "    rng = np.random.default_rng(42)\n",
        "    sub_n = min(20000, X_train.shape[0])\n",
        "    idx_sub = rng.choice(X_train.shape[0], size=sub_n, replace=False)\n",
        "\n",
        "    X_train_sub = X_train[idx_sub]\n",
        "    y_train_sub = y_train[idx_sub]\n",
        "\n",
        "    y_train_sub_pred = knn.predict(X_train_sub, batch_size=128)\n",
        "    train_acc = accuracy_score(y_train_sub, y_train_sub_pred)\n",
        "\n",
        "    print(f\"Train Accuracy (20k subsample): {train_acc:.4f}\")\n",
        "    print(f\"Test  Accuracy:                 {test_acc:.4f}\")\n",
        "\n",
        "    results.append((k, train_acc, test_acc))\n",
        "\n",
        "# Store results in a DataFrame for display\n",
        "results_df = pd.DataFrame(results, columns=[\"k\", \"train_accuracy\", \"test_accuracy\"])\n",
        "print(\"\\nSummary of K values:\")\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dkGoj66m1t5S",
      "metadata": {
        "id": "dkGoj66m1t5S"
      },
      "source": [
        "Where does your model fit in the fitting graph? (Build at least one model with different hyperparameters and check for over/underfitting, pick the best model).\n",
        "\n",
        "For k = 1, the model is on the left side of the fitting curve\n",
        "Low bias, extremely high variance\n",
        "Nearly perfect memorization of training patterns, bad generalization\n",
        "\n",
        "For k = 15, the model is on the right side of the curve\n",
        "High bias, low variance\n",
        "Oversmoothed boundaries lead to accuracy drops\n",
        "\n",
        "For k = 7, the model is in the center (optimal zone)\n",
        "Balanced bias and variance\n",
        "Best generalization performance of the three models\n",
        "\n",
        "Therefore, K-Nearest Neighbors with k=7 provides the best model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v36q29v92uwy",
      "metadata": {
        "id": "v36q29v92uwy"
      },
      "source": [
        "What are the next models you are thinking of and why?\n",
        "\n",
        "Gaussian Naive Bayes: Our features are continuous (nutrients), and classes overlap; GNB often sets a strong baseline on numeric, moderately correlated features. Trains and predicts with tiny memory — ideal for our scale.\n",
        "\n",
        "Decision Tree can model non-linear interactions (e.g., high sugar and high fat lead to lower grade). Gives interpretable splits and feature importance which is useful to explain nutrition drivers.\n",
        "\n",
        "Linear SVM can maximize margin in high-dimensional numeric space; often robust with regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ObrM3YmQ3g3_",
      "metadata": {
        "id": "ObrM3YmQ3g3_"
      },
      "source": [
        "Conclusion section: What is the conclusion of your 1st model? What can be done to possibly improve it?\n",
        "\n",
        "Our first model was a K-Nearest Neighbors classifier implemented and trained on the scaled nutrient features. We evaluated multiple hyperparameters to understand the model's fit behavior.\n",
        "\n",
        "After trying k = 1, 7, and 15, we found that k=1 severely overfits (train=92.8%, test=50.6%), while k=15 underfits (train=56.2%, test=49.7%). The optimal value is k=7, which achieves the best tradeoff between bias and variance (train=61.3%, test=50.1%), placing the model near the middle of the fitting curve. Therefore, KNN(k=7) is selected as our first model.\n",
        "\n",
        "Why the model struggles: NutriScore categories share similar nutrient profiles (e.g., many “C” and “D” foods overlap in sugar/fat ranges). KNN relies strictly on Euclidean distance; if class clusters overlap, it cannot draw meaningful boundaries.\n",
        "\n",
        "\n",
        "What can be done to possibly improve the model?\n",
        "Add categorical and text-derived features: Parse ingredients, labels, categories, allergens into binary or frequency features. Nutrient-only data is too limited; text-based metadata will provide richer class signals\n",
        "\n",
        "Try different models: Gaussian Naive Bayes provides strong on continuous numeric data. Decision Tree captures sugar and fat interactions and rule-like splits. Linear or kernelized SVM via SGD handles high-dimensional dense features."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
